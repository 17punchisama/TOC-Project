# backend/crawl.py
from pathlib import Path
import re
import urllib.parse
import requests
import html
import csv


from typing import Dict, Any

# ‡πÇ‡∏Ñ‡∏£‡∏á‡∏™‡∏£‡πâ‡∏≤‡∏á path: <repo-root>/frontend/posters
BASE_DIR = Path(__file__).resolve().parents[1]
POSTER_DIR = BASE_DIR / "frontend" / "posters"
POSTER_DIR.mkdir(parents=True, exist_ok=True)

PUBLIC_PREFIX = "/posters"

HEADERS = {
    "User-Agent": ("Mozilla/5.0 (Windows NT 10.0; Win64; x64) "
                   "AppleWebKit/537.36 (KHTML, like Gecko) "
                   "Chrome/120.0.0.0 Safari/537.36"),
    "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8",
    "Accept-Language": "en-US,en;q=0.9",
    "Referer": "https://yflix.me/",
    "Connection": "keep-alive",
}

BASE_URL = "https://yflix.me/category/series/page/{}/"

# ‡πÄ‡∏Å‡πá‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏£‡∏ß‡∏° (id -> series_info)
series_dict: Dict[int, Dict[str, Any]] = {}
# map ‡∏à‡∏≤‡∏Å‡∏•‡∏¥‡∏á‡∏Å‡πå‡∏ã‡∏µ‡∏£‡∏µ‡∏™‡πå -> id (‡∏Å‡∏±‡∏ô‡∏™‡∏£‡πâ‡∏≤‡∏á id ‡πÉ‡∏´‡∏°‡πà‡∏ã‡πâ‡∏≥)
url_to_id: Dict[str, int] = {}
# running id ‡∏Ñ‡∏£‡∏±‡πâ‡∏á‡πÅ‡∏£‡∏Å‡πÄ‡∏ó‡πà‡∏≤‡∏ô‡∏±‡πâ‡∏ô
_next_id = 1

def extract_balanced_div_block(html, start_id):
    pattern = rf'<div[^>]+id="{start_id}"[^>]*>'
    match = re.search(pattern, html)
    if not match:
        return None

    start_pos = match.start()
    remaining_html = html[start_pos:]

    open_divs = 0
    end_pos = 0
    for match in re.finditer(r'</?div\b', remaining_html):
        if match.group() == '<div':
            open_divs += 1
        else:
            open_divs -= 1
        if open_divs == 0:
            end_pos = match.end()
            break

    return remaining_html[:end_pos] if end_pos > 0 else None


def normalize_img_url(url: str) -> str:
    """‡∏ï‡∏±‡∏î suffix -WxH ‡∏≠‡∏≠‡∏Å‡∏à‡∏≤‡∏Å‡∏£‡∏π‡∏õ‡∏¢‡πà‡∏≠‡∏¢ ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏û‡∏¢‡∏≤‡∏¢‡∏≤‡∏°‡∏Ç‡∏≠‡πÑ‡∏ü‡∏•‡πå full-size"""
    return re.sub(r"-\d+x\d+(\.\w+)$", r"\1", url)


def get_ext_from_url(url: str) -> str:
    """‡∏Ñ‡∏∑‡∏ô extension ‡∏à‡∏≤‡∏Å path; default ‡πÄ‡∏õ‡πá‡∏ô .jpg"""
    ext = Path(urllib.parse.urlparse(url).path).suffix.lower()
    if ext in [".jpg", ".jpeg", ".png", ".gif", ".webp"]:
        return ".jpg" if ext == ".jpeg" else ext
    return ".jpg"


def _poster_public_path(stem: str, ext: str) -> str:
    """‡∏Ñ‡∏∑‡∏ô path ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö frontend ‡πÄ‡∏ä‡πà‡∏ô /posters/123.jpg"""
    return f"{PUBLIC_PREFIX}/{stem}{ext}"


def _poster_abs_path(stem: str, ext: str) -> Path:
    """‡∏Ñ‡∏∑‡∏ô path ‡∏à‡∏£‡∏¥‡∏á‡∏ö‡∏ô‡πÄ‡∏Ñ‡∏£‡∏∑‡πà‡∏≠‡∏á‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡πÄ‡∏Å‡πá‡∏ö‡πÑ‡∏ü‡∏•‡πå‡πÇ‡∏õ‡∏™‡πÄ‡∏ï‡∏≠‡∏£‡πå"""
    return POSTER_DIR / f"{stem}{ext}"


def save_original_if_needed(img_url: str, id_stem: str) -> str:
    """
    ‡∏î‡∏≤‡∏ß‡∏ô‡πå‡πÇ‡∏´‡∏•‡∏î‡πÑ‡∏ü‡∏•‡πå‡∏ï‡πâ‡∏ô‡∏â‡∏ö‡∏±‡∏ö (‡πÑ‡∏°‡πà‡πÅ‡∏õ‡∏•‡∏á) ‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡∏ñ‡πâ‡∏≤‡∏¢‡∏±‡∏á‡πÑ‡∏°‡πà‡∏°‡∏µ‡πÑ‡∏ü‡∏•‡πå
    ‡∏Ñ‡∏∑‡∏ô path ‡∏™‡∏≤‡∏ò‡∏≤‡∏£‡∏ì‡∏∞‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö frontend ‡πÄ‡∏ä‡πà‡∏ô /posters/123.jpg
    """
    ext = get_ext_from_url(img_url)
    abs_path = _poster_abs_path(id_stem, ext)
    public = _poster_public_path(id_stem, ext)

    if abs_path.exists():
        # ‡∏°‡∏µ‡πÑ‡∏ü‡∏•‡πå‡πÅ‡∏•‡πâ‡∏ß ‡πÑ‡∏°‡πà‡πÇ‡∏´‡∏•‡∏î‡∏ã‡πâ‡∏≥
        return public

    with requests.get(img_url, stream=True, headers=HEADERS, timeout=30) as r:
        r.raise_for_status()
        with open(abs_path, "wb") as f:
            for chunk in r.iter_content(8192):
                if chunk:
                    f.write(chunk)
    return public


def _upsert_series(sid: int, *, title: str, href: str, poster_url: str) -> Dict[str, Any]:
    """‡∏≠‡∏±‡∏õ‡πÄ‡∏î‡∏ï/‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ã‡∏µ‡∏£‡∏µ‡∏™‡πå 1 ‡πÄ‡∏£‡∏∑‡πà‡∏≠‡∏á (‡πÑ‡∏°‡πà‡πÄ‡∏Å‡πá‡∏ö page, ‡πÅ‡∏•‡∏∞‡πÑ‡∏°‡πà‡∏™‡∏£‡πâ‡∏≤‡∏á id ‡πÉ‡∏´‡∏°‡πà‡∏ñ‡πâ‡∏≤‡πÄ‡∏Ñ‡∏¢‡∏°‡∏µ‡∏≠‡∏¢‡∏π‡πà)"""
    # ‡∏Ç‡∏≠‡πÑ‡∏ü‡∏•‡πå full-size ‡∏ñ‡πâ‡∏≤‡πÄ‡∏õ‡πá‡∏ô‡πÑ‡∏õ‡πÑ‡∏î‡πâ
    img_url_full = normalize_img_url(poster_url)

    # ‡πÄ‡∏ã‡∏ü‡πÇ‡∏õ‡∏™‡πÄ‡∏ï‡∏≠‡∏£‡πå (‡∏ñ‡πâ‡∏≤‡∏¢‡∏±‡∏á‡πÑ‡∏°‡πà‡∏°‡∏µ)
    try:
        poster_public = save_original_if_needed(img_url_full, str(sid))
    except Exception as e:
        # fallback ‡πÑ‡∏õ url ‡πÄ‡∏î‡∏¥‡∏°
        try:
            poster_public = save_original_if_needed(poster_url, str(sid))
        except Exception as e2:
            print(f"  x poster failed id={sid} full={e} fallback={e2}")
            poster_public = ""

    # series_info ‡∏ó‡∏µ‡πà‡πÄ‡∏Å‡πá‡∏ö‡∏à‡∏£‡∏¥‡∏á (‡πÑ‡∏°‡πà‡πÉ‡∏™‡πà page)
    info = {
        "id": sid,
        "title": title,
        "url": href,
        "poster": poster_public,
    }
    series_dict[sid] = info
    return info


def scrape_page(page: int) -> Dict[int, Dict[str, Any]]:
    """
    Crawl ‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡∏´‡∏ô‡πâ‡∏≤ page (1..17)
    - ‡πÑ‡∏°‡πà‡πÄ‡∏Å‡πá‡∏ö field 'page'
    - ‡∏ñ‡πâ‡∏≤ href ‡πÄ‡∏Ñ‡∏¢‡πÄ‡∏à‡∏≠‡πÅ‡∏•‡πâ‡∏ß -> ‡πÉ‡∏ä‡πâ id ‡πÄ‡∏î‡∏¥‡∏°, ‡∏≠‡∏±‡∏õ‡πÄ‡∏î‡∏ï‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•/‡πÇ‡∏õ‡∏™‡πÄ‡∏ï‡∏≠‡∏£‡πå‡πÉ‡∏´‡πâ‡∏ï‡∏£‡∏á
    - ‡∏ñ‡πâ‡∏≤ href ‡∏¢‡∏±‡∏á‡πÑ‡∏°‡πà‡πÄ‡∏Ñ‡∏¢ -> ‡∏≠‡∏≠‡∏Å id ‡πÉ‡∏´‡∏°‡πà (‡∏Ñ‡∏£‡∏±‡πâ‡∏á‡πÅ‡∏£‡∏Å‡πÄ‡∏ó‡πà‡∏≤‡∏ô‡∏±‡πâ‡∏ô)
    """
    global _next_id

    print(f"[CRAWL] page {page}")
    res = requests.get(BASE_URL.format(page), headers=HEADERS, timeout=30)
    if res.status_code != 200:
        print(f"‚ùå Failed to fetch page {page}: status {res.status_code}")
        return {}

    section_html = extract_balanced_div_block(res.text, "tdi_45")
    if not section_html:
        print(f"‚ùå No section found for id='tdi_45' on page {page}")
        return {}

    # ‡∏´‡∏≤ series entries
    series_entries = re.findall(
        r'<div class="td-module-thumb">\s*<a href="(?P<url>https://yflix\.me/series/[^"]+)"[^>]*title="(?P<title>[^"]+)".*?data-img-url="(?P<poster>[^"]+)"',
        section_html,
        re.DOTALL
    )
    print(f"ü•© Found {len(series_entries)} series entries")
    page_data: Dict[int, Dict[str, Any]] = {}

    for url, title, poster_url in series_entries:
        title = html.unescape(title.strip())
        poster_url = poster_url.strip()

        # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ß‡πà‡∏≤‡∏ã‡∏µ‡∏£‡∏µ‡∏™‡πå‡∏ô‡∏µ‡πâ‡πÄ‡∏Ñ‡∏¢‡∏°‡∏µ‡∏≠‡∏¢‡∏π‡πà‡∏´‡∏£‡∏∑‡∏≠‡∏¢‡∏±‡∏á
        if url in url_to_id:
            sid = url_to_id[url]
        else:
            sid = _next_id
            url_to_id[url] = sid
            _next_id += 1

        print(f"üü¢ Title: {title}")
        print(f"üîó URL: {url}")
        print(f"üñºÔ∏è Poster: {poster_url}")
        print(f"#Ô∏è‚É£ Index: {sid}")

        poster_url_full = normalize_img_url(poster_url)
        try:
            local_poster_path = save_original_if_needed(poster_url_full, str(sid))
        except Exception as e:
            print(f"‚ùå Failed to download poster for {title}: {e}")
            local_poster_path = ""

        print(f"üíæ Saved to: {local_poster_path}")

        # ‡∏™‡∏£‡πâ‡∏≤‡∏á/‡∏≠‡∏±‡∏õ‡πÄ‡∏î‡∏ï‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÉ‡∏ô dict
        info = {
            "id": sid,
            "title": title,
            "url": url,
            "poster": local_poster_path,
        }
        series_dict[sid] = info
        page_data[sid] = info


    print(f"  ‚úì page {page} -> {len(page_data)} ‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£")
    return page_data


def scrape_all(total_pages: int = 17) -> Dict[int, Dict[str, Any]]:
    """
    Crawl ‡∏ó‡∏∏‡∏Å‡∏´‡∏ô‡πâ‡∏≤ 1..total_pages
    - ‡πÑ‡∏°‡πà‡πÄ‡∏Å‡πá‡∏ö field 'page'
    - ‡πÑ‡∏°‡πà‡∏™‡∏£‡πâ‡∏≤‡∏á id ‡πÉ‡∏´‡∏°‡πà‡∏ã‡πâ‡∏≥ (href ‡∏ä‡∏µ‡πâ id ‡πÄ‡∏î‡∏¥‡∏°‡πÄ‡∏™‡∏°‡∏≠)
    """
    all_data: Dict[int, Dict[str, Any]] = {}
    for p in range(1, total_pages + 1):
        all_data.update(scrape_page(p))
    return 

